{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "\n",
    "Refrencing code from\n",
    "\n",
    "https://freedium.cfd/https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/02/implementing-logistic-regression-from-scratch-using-python/\n",
    "\n",
    "Article help from\n",
    "\n",
    "https://arxiv.org/pdf/1609.04747.pdf\n",
    "\n",
    "https://iamtrask.github.io/2015/07/27/python-network-part2/\n",
    "\n",
    "https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html\n",
    "\n",
    "\n",
    "\n",
    "Note: epoch was set to 2000 across the board for initial testing, this causes the code to take around 60-90 seconds to run. You don't have that kind of time, so I changed them to about 800 which didn't seem to effect it much at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Data source: https://www.kaggle.com/datasets/luvharishkhati/heart-disease-patients-details/data\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "# X is the input data in the shape (m x n) aka (examples x features)\n",
    "X = df.drop(columns=['result']).values\n",
    "# scale the data between 0 and 1\n",
    "transformer = Normalizer(norm='max').fit(X)\n",
    "X = transformer.transform(X)\n",
    "\n",
    "# y is true/target value (this can only be 0 or 1) \n",
    "y = df['result'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs a value between 0 and 1\n",
    "# z is an integer equivalnet to \n",
    "# weight1*data1 + weight2*data2 + ... + weightn + datan + bias\n",
    "# in the form np.dot(X, w) + b)\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a value representative of the accuracy of our guesses\n",
    "# y = true value (either 0 or 1)\n",
    "# y_pred = probability of y being 1. Ranges from 0 to 1\n",
    "# More correct predictions return closer to 0 \n",
    "# Less correct predictions approach infinity\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the gradient\n",
    "def gradients(X, y, y_pred):\n",
    "   \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # dw is the partial derivative of the loss function with respect to w \n",
    "    dw = (1/m)*np.dot(X.T, (y_pred - y))\n",
    "    \n",
    "    # db is the partial derivative of the Loss function with respect to b\n",
    "    db = (1/m)*np.sum((y_pred - y)) \n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, bs, epochs, lr):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # Reshaping y\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Mini batch gradient descent\n",
    "        # Performs an update for every mini batch, number of mini batches\n",
    "        # is defined by (m-1)//bs\n",
    "        for i in range((m-1)//bs + 1):\n",
    "            \n",
    "            # Defining batches\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Calculating hypothesis/prediction\n",
    "            y_pred = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss with refrence to parameters\n",
    "            dw, db = gradients(xb, yb, y_pred)\n",
    "            \n",
    "            # Updating the parameters\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Adding loss/cost to the list\n",
    "        losses.append( loss(y, sigmoid(np.dot(X, w) + b)) )\n",
    "        \n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGD(X, y, epochs, lr):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    # Reshaping y\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Stochastic Gradient Descent\n",
    "        for i in range(m-1):\n",
    "            \n",
    "            # Calculating hypothesis/prediction\n",
    "            y_pred = sigmoid(np.dot(X[i:i+1], w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss with refrence to parameters\n",
    "            dw, db = gradients(X[i:i+1], y[i], y_pred) \n",
    "            \n",
    "            # Updating the parameters\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        losses.append( loss(y, sigmoid(np.dot(X, w) + b)) )\n",
    "        \n",
    "    # returning weights, bias and losses(List).\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes the y_pred range into binary predictions\n",
    "def predict(X, w, b):\n",
    "\n",
    "    # Making predictions\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "\n",
    "    # if y_pred >= 0.5 round up to 1\n",
    "    # if y_pred < 0.5 round down to 0\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meaures the accuracy of the model from 0-1 (percent correct predictions)\n",
    "# y is our array of target values\n",
    "# (y_pred was adjusted to be ONLY 0 or 1 by the predict function, it\n",
    "# is no longer a range)\n",
    "def accuracy(y, y_pred):\n",
    "    return np.sum(y == y_pred) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures correct/false positives and negatives\n",
    "def accuracy_measure(y, y_pred):\n",
    "\n",
    "    # True Positive (TP): Predicted True and True in reality.\n",
    "    # True Negative (TN): Predicted False and False in reality.\n",
    "    # False Positive (FP): Predicted True and False in reality.\n",
    "    # False Negative (FN): Predicted False and True in reality.\n",
    "\n",
    "    TP = np.sum(np.logical_and(y == 1, y_pred == 1))\n",
    "    TN = np.sum(np.logical_and(y == 0, y_pred == 0))\n",
    "    FP = np.sum(np.logical_and(y == 0, y_pred == 1))\n",
    "    FN = np.sum(np.logical_and(y == 1, y_pred == 0))\n",
    "\n",
    "    return TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy minibatch\n",
      "0.7074074074074074\n",
      "Accuracy SGD\n",
      "0.725925925925926\n",
      "Accuracy minibatch TP,TN,FP,FN\n",
      "(71, 120, 30, 49)\n",
      "Accuracy SGD TP,TN,FP,FN\n",
      "(73, 123, 27, 47)\n",
      "F1 batch minibatch\n",
      "0.6425339366515838\n",
      "F1 batch SGD\n",
      "0.6636363636363637\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "w, b, l = train(X, y, bs=10, epochs=800, lr=0.05) # Mini-batch gradient descent\n",
    "w2, b2, l2 = trainSGD(X, y, epochs=800, lr=0.05) # Stochastic gradient descent\n",
    "\n",
    "# SGD performs very marginally better under the same epochs and learning rate\n",
    "# It is, however, WAY slower...\n",
    "\n",
    "# Decreasing batch size (bs) improves the accuracy of the model but slows its training\n",
    "# Increasing epochs usually improves the accuracy of the model but slows its training\n",
    "# Adjusting (increasing for SGD!) learning rate (lr) may help fine tune the model\n",
    "\n",
    "# What is our accuracy\n",
    "print(\"Accuracy minibatch\")\n",
    "print(accuracy(y, y_pred=predict(X, w, b)))\n",
    "print(\"Accuracy SGD\")\n",
    "print(accuracy(y, y_pred=predict(X, w2, b2)))\n",
    "\n",
    "print(\"Accuracy minibatch TP,TN,FP,FN\")\n",
    "print(accuracy_measure(y, y_pred=predict(X, w, b)))\n",
    "print(\"Accuracy SGD TP,TN,FP,FN\")\n",
    "print(accuracy_measure(y, y_pred=predict(X, w2, b2)))\n",
    "\n",
    "print(\"F1 batch minibatch\")\n",
    "print(f1_score(y, predict(X, w, b)))\n",
    "print(\"F1 batch SGD\")\n",
    "print(f1_score(y, predict(X, w2, b2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the values used in the training set, bs=10, epochs=2000, lr=0.05, the accuracy of the model is about 0.72 for mini-batch and .76 for SGD. This was the best I could get after tinkering around with the values, it isn't great, but it's not terrible. Why isn't it better? After all, our data should be sufficient, it's definitely has enough to visually observe the trends in our exploratory data analysis. Plus, our data is normalized at the start of this process so there shouldn't be an issue there. \n",
    "\n",
    "My best guess is that the model is too complex. With 13 columns, realisitcally the model should get more time, more batches, and a more optimized learning rate to try and find the best weights and bias. I played with the epochs, bs and lr a lot but \"played with\" isn't exactly a watertight methodology. Either way, when I printed out the final weights in testing, they were all in the same general range. Intuitively this doesn't make sense, we can clearly see that some data trends *much* more strongly with our result than other data. The best way to deal with this would probably be to drop less relevant features to reduce noise. For example, fasting_blood_sugar trends with -0.016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGD_momentum(X, y, epochs, lr, momentum):\n",
    "     \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights, bias and velocity terms to zero\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    v_w = np.zeros((n,1))\n",
    "    v_b = 0\n",
    "    \n",
    "    # Reshaping y\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        # Stochastic Gradient Descent\n",
    "        for i in range(m-1):\n",
    "            \n",
    "            # Calculating hypothesis/prediction\n",
    "            y_pred = sigmoid(np.dot(X[i:i+1], w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss with refrence to parameters\n",
    "            dw, db = gradients(X[i:i+1], y[i], y_pred) \n",
    "            \n",
    "            # Updating the parameters\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "\n",
    "            # Update with momentum\n",
    "            v_w = momentum * v_w + lr * dw  # Update velocity for weights\n",
    "            v_b = momentum * v_b + lr * db  # Update velocity for bias\n",
    "            \n",
    "            # Update weights and bias\n",
    "            w -= v_w\n",
    "            b -= v_b\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        losses.append( loss(y, sigmoid(np.dot(X, w) + b)) )\n",
    "        \n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGD_Adagrad(X, y, epochs, lr):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights, bias and accumulated square gradient to zeros\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    G_w = np.zeros((n, 1))\n",
    "    G_b = 0\n",
    "\n",
    "    # For storing loss\n",
    "    losses = []\n",
    "\n",
    "    # Reshaping y\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m-1):\n",
    "            \n",
    "            # Making predictions\n",
    "            y_pred = sigmoid(np.dot(X[i:i+1], w) + b)\n",
    "            \n",
    "            # Getting the gradients of loss with refrence to parameters\n",
    "            dw, db = gradients(X[i:i+1], y[i], y_pred) \n",
    "            \n",
    "            # Updating weights and bias\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "\n",
    "            # Update accumulated square gradient\n",
    "            G_w += dw**2\n",
    "            G_b += db**2\n",
    "            \n",
    "            # Update weights and bias using Adagrad\n",
    "            w -= (lr / (np.sqrt(G_w) + 1e-8)) * dw\n",
    "            b -= (lr / (np.sqrt(G_b) + 1e-8)) * db\n",
    "        \n",
    "        # Adding loss/cost to the list\n",
    "        losses.append(loss(y, sigmoid(np.dot(X, w) + b)))\n",
    "        \n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy minibatch\n",
      "0.8185185185185185\n",
      "Accuracy SGD\n",
      "0.7407407407407407\n",
      "Accuracy minibatch TP,TN,FP,FN\n",
      "(92, 129, 21, 28)\n",
      "Accuracy SGD TP,TN,FP,FN\n",
      "(73, 127, 23, 47)\n",
      "F1 batch minibatch\n",
      "0.7896995708154506\n",
      "F1 batch SGD\n",
      "0.6759259259259259\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent with momentum\n",
    "w, b, l = trainSGD_momentum(X, y, epochs=800, lr=0.05, momentum=0.9) \n",
    "w2, b2, l2 = trainSGD_Adagrad(X, y, epochs=800, lr=0.05) \n",
    "\n",
    "# What is our accuracy\n",
    "print(\"Accuracy minibatch\")\n",
    "print(accuracy(y, y_pred=predict(X, w, b)))\n",
    "print(\"Accuracy SGD\")\n",
    "print(accuracy(y, y_pred=predict(X, w2, b2)))\n",
    "\n",
    "print(\"Accuracy minibatch TP,TN,FP,FN\")\n",
    "print(accuracy_measure(y, y_pred=predict(X, w, b)))\n",
    "print(\"Accuracy SGD TP,TN,FP,FN\")\n",
    "print(accuracy_measure(y, y_pred=predict(X, w2, b2)))\n",
    "\n",
    "print(\"F1 batch minibatch\")\n",
    "print(f1_score(y, predict(X, w, b)))\n",
    "print(\"F1 batch SGD\")\n",
    "print(f1_score(y, predict(X, w2, b2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The simple answer for why we use the gradient descent improvements is because they are improvements. The accuracy of the model is better though the time it takes to run increases somewhat. As often is the case, it's a balancing act, is that extra training time worth the increase in model accuracy. In our case, ~15 seconds increase is worth it because 15 seconds isn't that much. Proportionally that is a lot however, so be mindful on larger data sets and weaker computers. (Note that most of this increase in time comes from the fact that both of the optimizations are perfomred on SGD which takes WAY longer than mini batch.)\n",
    "\n",
    "In this case, Adagrad seems to be a more marginal improvement while momentum makes the model almost a full 8% better! Given the data we are working with, this is likely because it is so noisy, see EDA for a visual refresher. The data is also \"weird\" since many of the features are quantized. Adagrad can be sensitive to noise and situations where the landscape does weird stuff like just from 3 to 6 to 7 exclusively. Since the historical gradient may not be the most informative if it's jumping around like a bunch of V's. Momentum on the other hand is suited to escape areas of weird noise and won't suffer from trying to adapt to historical gradient.\n",
    "\n",
    "For measuring accuracy, the counts of True Positive, True Negative, False Positive and False Negative were included. This works for two reasons, one it helps inform the threshold, if your false positive is really high and your false negative is really low, it would be a good idea to move the threshold down and vice versa. Second, it informs our use of the F1 score as a second method of measurement.\n",
    "\n",
    "\"The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "An F1 score punishes extreme values more. Ideally, an F1 Score could be an effective evaluation metric in the following classification scenarios:\n",
    "\n",
    "When FP and FN are equally costly\n",
    "Adding more data doesn’t effectively change the outcome effectively\n",
    "TN is high (like with flood predictions, cancer predictions, etc.) \"\n",
    "\n",
    "This is perfect! We fit all of these criteria. Our FN and FP values are about equal and TN is high. Apparently this is common in medical models due to its effectiveness in handling imbalanced datasets, a common characteristic in medical data where one class (e.g., disease presence) may be significantly less frequent than the other. F1 score considers both precision and recall, so it's for situations where false positives and false negatives carry different clinical implications. You don't want to end up being told you have heart disease when you don't! (Though the other way around would be worse...) The F1 score is ideal for measuing the quality of models that assist doctors in making decisions, perfect for this example.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
